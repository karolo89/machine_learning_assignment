---
title: "Modeling Assignment 1, Hendrik's benchmark"
author: "Karol Orozco & Charles Hanks" 
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Setup

```{r, warning= FALSE, message=FALSE}
# add your libraries
library(tidyverse)
library(caret)
library(rpart)
library(glmnet)
<<<<<<< HEAD

=======
>>>>>>> 6fce7ee249f70b31a4e576fa95332f81bb79c806

wine = read_rds("pinot.rds")

```


## Feature Engineering

```{r}
# create some cool features. Make sure you add comments so I know what you are trying to accomplish!
wine_words <- function(df, j = 1000, stem=F){ 
  library(tidytext)
  library(SnowballC)
  data(stop_words)
  words <- df %>%
    unnest_tokens(word, description) %>%
    anti_join(stop_words) %>% # get rid of stop words
    filter(!(word %in% c("wine","pinot","vineyard", "price", "points")))
  
  if(stem){
    words <- words %>% 
      mutate(word = wordStem(word))
  }
  
  words <- words %>% 
    count(id, word) %>% 
    group_by(id) %>% 
    mutate(exists = (n>0)) %>% 
    ungroup %>% 
    group_by(word) %>% 
    mutate(total = sum(n)) %>% 
    filter(total > j) %>% 
    pivot_wider(id_cols = id, names_from = word, values_from = exists, values_fill = list(exists=0)) %>% 
    right_join(select(df,id,province)) %>% 
    mutate(across(-province, ~replace_na(.x, F)))
}

wino <- wine_words(wine, j=200, stem=F)

#bringing back numerical features from original dataset to wino:
wino = wino %>% left_join(select(wine, id, price, points, year), by = "id") 
  

#center and scale points: 
wino = wino %>% select(points) %>% preProcess(method = c("center", "scale")) %>% predict(wino)

#creating year as factor feature, logprice: 
wino = wino %>% mutate(lprice = log(price))


  
#removing original price                    
wino = wino %>% select(-price)


  
#removing original price                    
wino = wino %>% select(-price)


#binning year and and price: 
wino = wino %>% 
  mutate(price_f = case_when(
    price < 16 ~ "low",
    price >= 16 & price < 41 ~ "med",
    price >= 41 ~ "high"
  ), 
   year_f = case_when(
    year < 2005 ~ "old",
    year >= 2005 & year < 2011 ~ "recent",
    year >= 2011 ~ "current"
  ))

#adding cost per point feature, removing id column: 
wino = wino %>% select(-id, -price)


```


Running a logistic regression model to select words that are of high impact to NY: 
```{r}
wine2 <-  read_rds("pinot.rds") %>% 
  mutate(province = as.numeric(province=="New_York")) 


wino2 <- wine_words(wine2, j = 50)

wine_index <- createDataPartition(wino2$province, p = 0.80, list = FALSE)

train <- wino2[ wine_index, ]
test <- wino2[-wine_index, ]

control <- trainControl(method = "cv", number = 5)

fit.ny <- train(province ~ .,
             data = train, 
             trControl = control,
             method = "glm",
             family = "binomial")

#show the odds ratios for top coefficients
odds_ratio <- exp(coef(fit.ny$finalModel))
ny_words = data.frame(name = names(odds_ratio), odds_ratio = round(odds_ratio,2)) %>%  
  arrange(desc(odds_ratio))

#cleaning up column of words
ny_words = ny_words %>% mutate(name = gsub("TRUE", "", name)) %>% mutate(name = gsub("`\\\\`", "", name)) %>% mutate(name = gsub("\\\\``", "", name))


ny_words$name
features = colnames(wino)



length(features)

nyc = ny_words$name[1:185]

nyc = setdiff(nyc, features)[1:30]


nyc_words <- wine %>%
    unnest_tokens(word, description) %>%
    anti_join(stop_words) %>% 
    count(id, word) %>% 
    group_by(id) %>% 
    mutate(exists = (n>0)) %>% 
    ungroup %>% 
    group_by(word) %>% 
    mutate(total = sum(n)) %>% 
    filter(word %in% nyc) %>% 
    pivot_wider(id_cols = id, names_from = word, values_from = exists, values_fill = list(exists=0)) %>% 
    right_join(select(wine,id,province)) %>% 
    mutate(across(-province, ~replace_na(.x, F)))

nyc_words = nyc_words %>% arrange(id) %>% select(-province)

wino = wino %>% inner_join(nyc_words, by = 'id')

wino = wino %>% select(-id)


```




=======

## Specification

```{r}
#Model: ELASTIC NET REGRESSION
#Tuning parameters: 
#   1. lambda - regularization/penalty enforcement to minimize prediction error)
#   2, alpha  - mixing parameter between 0 and 1, where alpha = 0 is ridge regression, and alpha = 1 is lasso regression)

set.seed(504) 

ctrl <- trainControl(method = "cv", number = 5)

wine_index <- createDataPartition(wino$province, p = 0.80, list = FALSE)
train <- wino[ wine_index, ]
test <- wino[-wine_index, ]


fit <- train(province ~ .,
             data = train, 
             method = "glmnet",
             tuneGrid = expand.grid(alpha=seq(0,1,length=10),
                                    lambda = seq(0.0001,0.2,
                                    length=20)),
             trControl = ctrl,
             metric = "Kappa")

confusionMatrix(predict(fit, test),factor(test$province))




```

<<<<<<< HEAD
```{r}
varImp(fit) 
?varImp()
  
```
=======



>>>>>>> 6fce7ee249f70b31a4e576fa95332f81bb79c806

## Best model

```{r}
# Here are a few lines to inspect your best model. Add some comments about optimal hyperparameters.
print(fit)
print(fit$bestTune)

```


## Re-fit and evaluation

```{r}
# the "method" below should match the one you chose above. 
<<<<<<< HEAD

set.seed(504) # I will choose a different seed for evaluation
=======
set.seed(411) # I will choose a different seed for evaluation
>>>>>>> 6fce7ee249f70b31a4e576fa95332f81bb79c806

wine_index <- createDataPartition(wino$province, p = 0.80, list = FALSE)
train <- wino[ wine_index, ]
test <- wino[-wine_index, ]

# example spec for knn
fit_final <- train(province ~ .,
             data = train, 
             method = "glmnet",
             tuneGrid=fit$bestTune) 
# The last line means we will fit a model using the best tune parameters your CV found above.

confusionMatrix(predict(fit_final, test),factor(test$province))
```

