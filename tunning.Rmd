---
title: "Logistic regression and Random Forest"
author: "Charles & Karol"
date: "2023-02-17"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

## Setup


# add your libraries
library(tidyverse)
library(caret)
library(rpart)
library(randomForest)

wine = readRDS(gzcon(url(
"https://github.com/karolo89/machine_learning_assignment/raw/main/pinot.rds")))

```


## Feature Engineering

```{r}
# create some cool features. Make sure you add comments so I know what you are trying to accomplish!

wine_words <- function(df, j = 1000, stem=F){ 
  library(tidytext)
  library(SnowballC)
  data(stop_words)

  words <- df %>%
    unnest_tokens(word, description) %>%
    anti_join(stop_words) %>% # get rid of stop words
    filter(!(word %in% c("wine","pinot","vineyard")))
  
  if(stem){
    words <- words %>% 
      mutate(word = wordStem(word))
  }
  
  words <- words %>% 
    count(id, word) %>% 
    group_by(id) %>% 
    mutate(exists = (n>0)) %>% 
    ungroup %>% 
    group_by(word) %>% 
    mutate(total = sum(n)) %>% 
    filter(total > j) %>% 
    pivot_wider(id_cols = id, names_from = word, values_from = exists, values_fill = list(exists=0)) %>% 
    right_join(select(df,id,province)) %>% 
    mutate(across(-province, ~replace_na(.x, F)))
}

wino <- wine_words(wine, j=500, stem=F)

#joining wino back with price, points, year for more feature engineering:
wino2 = wino %>% left_join(select(wine, id, price, points, year), by = "id")

#adding year as factor col, log of price:
wino2 = wino2 %>% mutate(year_f = as.factor(year), 
                         lprice = log(price))

#binning year and and price: 
wino2 = wino2 %>% 
  mutate(price_f = case_when(
    price < 16 ~ "low",
    price >= 16 & price < 41 ~ "med",
    price >= 41 ~ "high"
  ), 
   year_f = case_when(
    year < 2005 ~ "old",
    year >= 2005 & year < 2011 ~ "recent",
    year >= 2011 ~ "current"
  ))

preProcess(wino2, method = "center", "scale", "pca")


head(wino2)

```

## Specification

```{r}


control <- trainControl(method='repeatedcv', 
                        number=10, 
                        repeats=3,
                        search = 'random')

#Random generate 15 mtry values with tuneLength = 15
set.seed(504)
fit  <- train(province ~ .,
                   data = train,
                   method = 'rf',
                   metric = "Kappa",
                   tuneLength  = 15, 
                   trControl = control)

confusionMatrix(predict(fit, test),factor(test$province))

```



## Best model

```{r}
# Here are a few lines to inspect your best model. Add some comments about optimal hyperparameters.
print(fit)
print(fit$bestTune)
```


## Re-fit and evaluation

```{r}
# the "method" below should match the one you chose above. 

set.seed(504) # I will choose a different seed for evaluation

wine_index <- createDataPartition(wino2$province, p = 0.80, list = FALSE)
train <- wino2[ wine_index, ]
test <- wino2[-wine_index, ]

# example spec for knn
fit_final <- train(province ~ .,
             data = train, 
             method = "rf",
             tuneGrid=fit$bestTune) 

# The last line means we will fit a model using the best tune parameters your CV found above.

confusionMatrix(predict(fit_final, test),factor(test$province))
```



