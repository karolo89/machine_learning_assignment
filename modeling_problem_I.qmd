---
title: "Modeling Problem I"
author: "Karol Orozco & Charles Hanks"
format: pdf
---

## Predicting Province

```{r setup, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)

library(tidyverse)
library(formatR)
library(moderndive)
library(skimr)
library(caret)

wine_pinot <- readRDS(gzcon(url(
  "https://github.com/karolo89/machine_learning_assignment/raw/main/pinot.rds")))
```

```{r}

#adding log price column
pinot <- wine_pinot %>% 
  mutate(lprice = log(price))

pinot <- pinot %>%
  mutate(id = as.factor(id))%>%
  mutate(year = as.factor(year))

summary(pinot)

```

**Preliminary EDA, Feature Engineering Brainstorm, Initial Thoughts**

```{r}
pinot %>% 
  group_by(province) %>% 
  summarize(prov_freq = n(), 
            percent_of_ds = round(prov_freq/8380,2))

#nearly half of wines are californian, good to know... 

pinot %>% 
  filter(str_detect(description, "[Oo]ak")) %>% 
  nrow()

#1301/8380 have the word oak in description 

pinot %>% filter(str_detect(description, "[Oo]ak")) %>% 
  group_by(province) %>% summarize(prov_freq = n(), 
                                   oak_perc = round(prov_freq/1301,2))

#it is likely California or Oregon if there is oak in the description

#some french language patterns to think about developing a regex from: 
# "_de_" / "d'"
# "name-name"
# accented letters: "é","ô",
# "St."

pinot %>% 
  group_by(province) %>% 
  summarize(avgPrice = mean(price), 
            avgPoints = mean(points))
# Burgundy wines are on average significantly more expensive...
# and casablanca valley wines on average have the lowest price and score.  

#which wines do people recommend waiting before drinking? i.e "drink from XXXX" 

#some words to check out: "edge","tannins","dense","firm", oregon pinot is fruity. 

province_vec = c("Burgundy", "California", "Casablanca_Valley","Marlborough", 
                 "New_York", "Oregon")

for(i in province_vec){
  plot = ggplot(pinot %>% 
                  filter(province == i), aes(x = as.integer(year), y = points)) +      
    geom_point(alpha =.5, color = "red4") + 
    ggtitle(i)+
    theme_minimal()
  
  print(plot)
}

for(i in province_vec){
  plot2 = ggplot(pinot %>% 
                   filter(province == i), aes(x = as.integer(year))) + 
    geom_histogram(binwidth =3, fill = "red4") + 
    ggtitle(i)+
    theme_minimal()
  print(plot2)
}

#Some findings from viz:
#california pinot noir production did not begin until ~2008, then exploded! 
#before year 2000, likely to be oregon 
#burgundy pinots score high around 2005, 
#after almost no burgundy pinots between 2000 and 2005
#California pinot game WAY STRONG between 2010 and 2015
#New York pinot score high between 2008 and 2015 
#What happened around 2014?? Counts drop across provinces....
```

## Preprocessing (3pts)

1.  Preprocess the dataframe that you created in the previous question using centering and scaling of the numeric features
2.  Create dummy variables for the year factor column

```{r}

```

## Running KNN (5pts)

1.  Split your data into an 80/20 training and test set
2.  Use Caret to run a KNN model that uses your engineered features to predict province

-   use 5-fold cross validated subsampling
-   allow Caret to try 15 different values for K

3.  Display the confusion matrix on the test data

```{r}

```

## Kappa (2pts)

Is this a good value of Kappa? Why or why not?

**Answer:** (write your answer here)

## Improvement (2pts)

Looking at the confusion matrix, where do you see room for improvement in your predictions?

**Answer:** (write your answer here)

## KNN Model, 5 fold CV resampling: 

```{r}
w = wine_pinot %>% mutate(lprice = log(price),
                    fyear = as.factor(year),
                    oak = as.integer(str_detect(description, "[Oo]ak")),
                    earth = as.integer(str_detect(description, "[Ee]arth")),
                    cherry = as.integer(str_detect(description, "[Cc]herry")),
                    choc = as.integer(str_detect(description, "[Cc]hocolate")),
                    acidity = as.integer(str_detect(description, "[Aa]cidity")),
                    nose = as.integer(str_detect(description, "[Nn]ose")),
                    palate = as.integer(str_detect(description, "[Pp]alate")),
                    chocolate = as.integer(str_detect(description,"[Cc]hocolate")),
                    tart = as.integer(str_detect(description,"[Tt]art")),
                    brisk = as.integer(str_detect(description,"[Bb]risk")),
                    bramble = as.integer(str_detect(description,"[Bb]ramble")),
                    aging = as.integer(str_detect(description, "[Aa]ging")),
                    savory =as.integer(str_detect(description, "[Ss]avory")),
                    clover = as.integer(str_detect(description, "[Cc]love")),
                    aromas = as.integer(str_detect(description, "[Aa]romas")),
                    fruits = as.integer(str_detect(description, "[Ff]ruits")),
                    nose = as.integer(str_detect(description, "[Nn]ose")),
                    points_greater_95 = points >=95, 
                    points_less_90 = points <= 90,
                    price_greater_4 = lprice >= 4, 
                    price_between_4_3 = lprice < 4 & lprice >= 3, 
                    price_less_3 = lprice < 3,
                    before_2010 = year < 2010, 
                    beween_2010_2015 = (year >=2010 & year <= 2015),
                    between_2015_2020 = (year > 2015 & year <= 2020)) %>%
                    select(-id,-price,-description)

```

```{r}
set.seed(504)

wine_index <- createDataPartition(w$province, p = 0.8, list = FALSE)
train <- w[ wine_index, ]
test <- w[-wine_index, ]

control <- trainControl(method = "cv", number = 5)

fit <- train(province ~ .,             
	data = train,              
	method = "knn",             
	tuneLength = 15,             
	trControl = control)

fit

confusionMatrix(predict(fit,test), factor(test$province)) 
```

## Group Activity: Naive Bayes Model

Use the top words by province to...


1\. Engineer more features that capture the essence of Casablanca, Marlborough and New York

2. Look for difference between California and Oregon

3\. Use what you find to run naive Bayes models that achieve a Kappa that approaches 0.5

```{r}
library(tidytext)
library(caret)
wine = wine_pinot
names(wine)[names(wine) == 'id'] = 'ID'
```

Document term matrix:

```{r}
df <- wine %>%
  unnest_tokens(word, description) %>%
  anti_join(stop_words) %>% # get rid of stop words
  filter(word != "wine") %>%
  filter(word != "pinot") %>%
  count(ID, word) %>% 
  group_by(ID) %>% 
  mutate(freq = n/sum(n)) %>% 
  mutate(exists = (n>0)) %>% 
  ungroup %>% 
  group_by(word) %>% 
  mutate(total = sum(n))
```

Pivot wide and rejoin with wine:

```{r}
wino <- df %>% 
  filter(total > 900) %>% 
  pivot_wider(id_cols = ID, names_from = word, values_from = exists, values_fill = list(exists=0)) %>% 
  merge(select(wine,ID, province), all.y=TRUE) #%>% 
  #drop_na()

#wino <- merge(select(wine,ID, province), wino, by="ID", all.x=TRUE) %>%
#  arrange(ID)
#View(wino)
wino <- replace(wino, is.na(wino), FALSE)
```

Visualizing distribution to select distinct features for provinces:

```{r}
df %>% 
  left_join(select(wine, ID, province), by = "ID") %>% 
  count(province, word) %>%
  group_by(province) %>% 
  top_n(10,n) %>% 
  arrange(province, desc(n)) %>% 
  ggplot(aes(x = word, y = n, fill = province)) + geom_col() + coord_flip()
```

```{r}
wino = wino %>% select(ID, province, tart, plum, oak, bodied,black,nose,palate,ripe,cherry,tannins,drink)
```

train & test model:

```{r}
wine_index <- createDataPartition(wino$province, p = 0.80, list = FALSE)
train <- wino[ wine_index, ]
test <- wino[-wine_index, ]

fit <- train(province ~ .,
             data = train, 
             method = "naive_bayes",
             tuneGrid = expand.grid(usekernel = c(T,F), laplace = T, adjust = T),
             metric = "Kappa",
             trControl = trainControl(method = "cv"))


confusionMatrix(predict(fit, test),factor(test$province))

```

Creating more features

```{r}

features = wine %>% 
  mutate(aging = str_detect(description,"aging"),
         chocolate =  str_detect(description, "chocolate"),
         vineyard = str_detect(description, "vineyard")) %>% 
            select(ID,aging,chocolate,vineyard)

wino2 = wino %>%
  left_join(features, by = "ID")


```

Test 2

```{r}
wine_index <- createDataPartition(wino2$province, p = 0.80, list = FALSE)
train <- wino2[ wine_index, ]
test <- wino2[-wine_index, ]

fit <- train(province ~ .,
             data = train, 
             method = "naive_bayes",
             tuneGrid = expand.grid(usekernel = c(T,F), laplace = T, adjust = T),
             metric = "Kappa",
             trControl = trainControl(method = "cv"))
fit

confusionMatrix(predict(fit, test),factor(test$province))

#Higher kappa value, but now model is not predicting any of the sparse provinces
```
